{
    "sourceFile": "preprocessing.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1742726303582,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1742729855584,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,49 @@\n+## download packages\n+#nltk.download('punkt_tab')\n+#nltk.download('stopwords')\n+nltk.download('wordnet')\n+\n+## import libraries\n+import pandas as pd\n+import nltk\n+from nltk.tokenize import word_tokenize\n+from nltk.corpus import stopwords\n+from nltk.stem import WordNetLemmatizer\n+\n+\n+## read the data\n+df_raw = pd.read_csv('Data/Raw/Job_Frauds.csv', encoding='latin-1')\n+\n+\n+## split 'Job Location' into 'Country', 'State', 'City'\n+df = df_raw\n+split_location = df['Job Location'].str.split(', ', expand=True)\n+df['Country'] = split_location[0]  \n+df['State'] = split_location[1] \n+df['City'] = split_location[2] \n+df = df.drop(columns=['Job Location'])\n+\n+## convert strings to lower case\n+df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n+\n+## remove punctuations and symbols\n+df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n+\n+## tokenize, remove stop words, and lemmatize for columns 'Profile', 'Job_Description', 'Requirements', 'Job_Benefits'\n+# initialize stop words and lemmatizer\n+stop_words = set(stopwords.words('english'))\n+lemmatizer = WordNetLemmatizer()    \n+cols = ['Profile', 'Job_Description', 'Requirements', 'Job_Benefits']\n+\n+for col in cols:\n+    # apply tokenization \n+    df[col] = df[col].apply(lambda x: word_tokenize(x) if isinstance(x, str) and x.strip() != '' else [])\n+    # remove stop words\n+    df[col] = df[col].apply(lambda x: [word for word in x if word not in stop_words])\n+    # apply lemmatization\n+    df[col] = df[col].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n+\n+print(df)\n+\n+## save as csv\n+#df.to_csv('Data/Processed/processed_df.csv', index=False)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1742729905323,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,50 @@\n+## download packages\n+nltk.download('punkt_tab')\n+nltk.download('stopwords')\n+nltk.download('wordnet')\n+\n+## import libraries\n+import pandas as pd\n+import nltk\n+\n+from nltk.tokenize import word_tokenize\n+from nltk.corpus import stopwords\n+from nltk.stem import WordNetLemmatizer\n+\n+\n+## read the data\n+df_raw = pd.read_csv('Data/Raw/Job_Frauds.csv', encoding='latin-1')\n+\n+\n+## split 'Job Location' into 'Country', 'State', 'City'\n+df = df_raw\n+split_location = df['Job Location'].str.split(', ', expand=True)\n+df['Country'] = split_location[0]  \n+df['State'] = split_location[1] \n+df['City'] = split_location[2] \n+df = df.drop(columns=['Job Location'])\n+\n+## convert strings to lower case\n+df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n+\n+## remove punctuations and symbols\n+df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n+\n+## tokenize, remove stop words, and lemmatize for columns 'Profile', 'Job_Description', 'Requirements', 'Job_Benefits'\n+# initialize stop words and lemmatizer\n+stop_words = set(stopwords.words('english'))\n+lemmatizer = WordNetLemmatizer()    \n+cols = ['Profile', 'Job_Description', 'Requirements', 'Job_Benefits']\n+\n+for col in cols:\n+    # apply tokenization \n+    df[col] = df[col].apply(lambda x: word_tokenize(x) if isinstance(x, str) and x.strip() != '' else [])\n+    # remove stop words\n+    df[col] = df[col].apply(lambda x: [word for word in x if word not in stop_words])\n+    # apply lemmatization\n+    df[col] = df[col].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n+\n+print(df)\n+\n+## save as csv\n+#df.to_csv('Data/Processed/processed_df.csv', index=False)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1742730234349,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,111 +1,13 @@\n ## download packages\n-nltk.download('punkt_tab')\n-nltk.download('stopwords')\n-nltk.download('wordnet')\n-\n-## import libraries\n-import pandas as pd\n-import nltk\n-\n-from nltk.tokenize import word_tokenize\n-from nltk.corpus import stopwords\n-from nltk.stem import WordNetLemmatizer\n-\n-\n-## read the data\n-df_raw = pd.read_csv('Data/Raw/Job_Frauds.csv', encoding='latin-1')\n-\n-\n-## split 'Job Location' into 'Country', 'State', 'City'\n-df = df_raw\n-split_location = df['Job Location'].str.split(', ', expand=True)\n-df['Country'] = split_location[0]  \n-df['State'] = split_location[1] \n-df['City'] = split_location[2] \n-df = df.drop(columns=['Job Location'])\n-\n-## convert strings to lower case\n-df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n-\n-## remove punctuations and symbols\n-df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n-\n-## tokenize, remove stop words, and lemmatize for columns 'Profile', 'Job_Description', 'Requirements', 'Job_Benefits'\n-# initialize stop words and lemmatizer\n-stop_words = set(stopwords.words('english'))\n-lemmatizer = WordNetLemmatizer()    \n-cols = ['Profile', 'Job_Description', 'Requirements', 'Job_Benefits']\n-\n-for col in cols:\n-    # apply tokenization \n-    df[col] = df[col].apply(lambda x: word_tokenize(x) if isinstance(x, str) and x.strip() != '' else [])\n-    # remove stop words\n-    df[col] = df[col].apply(lambda x: [word for word in x if word not in stop_words])\n-    # apply lemmatization\n-    df[col] = df[col].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n-\n-print(df)\n-\n-## save as csv\n-#df.to_csv('Data/Processed/processed_df.csv', index=False)\n-## download packages\n #nltk.download('punkt_tab')\n #nltk.download('stopwords')\n-nltk.download('wordnet')\n-\n-## import libraries\n-import pandas as pd\n-import nltk\n-from nltk.tokenize import word_tokenize\n-from nltk.corpus import stopwords\n-from nltk.stem import WordNetLemmatizer\n-\n-\n-## read the data\n-df_raw = pd.read_csv('Data/Raw/Job_Frauds.csv', encoding='latin-1')\n-\n-\n-## split 'Job Location' into 'Country', 'State', 'City'\n-df = df_raw\n-split_location = df['Job Location'].str.split(', ', expand=True)\n-df['Country'] = split_location[0]  \n-df['State'] = split_location[1] \n-df['City'] = split_location[2] \n-df = df.drop(columns=['Job Location'])\n-\n-## convert strings to lower case\n-df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n-\n-## remove punctuations and symbols\n-df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n-\n-## tokenize, remove stop words, and lemmatize for columns 'Profile', 'Job_Description', 'Requirements', 'Job_Benefits'\n-# initialize stop words and lemmatizer\n-stop_words = set(stopwords.words('english'))\n-lemmatizer = WordNetLemmatizer()    \n-cols = ['Profile', 'Job_Description', 'Requirements', 'Job_Benefits']\n-\n-for col in cols:\n-    # apply tokenization \n-    df[col] = df[col].apply(lambda x: word_tokenize(x) if isinstance(x, str) and x.strip() != '' else [])\n-    # remove stop words\n-    df[col] = df[col].apply(lambda x: [word for word in x if word not in stop_words])\n-    # apply lemmatization\n-    df[col] = df[col].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n-\n-print(df)\n-\n-## save as csv\n-#df.to_csv('Data/Processed/processed_df.csv', index=False)\n-## download packages\n-#nltk.download('punkt_tab')\n-#nltk.download('stopwords')\n #nltk.download('wordnet')\n \n ## import libraries\n import pandas as pd\n import nltk\n+nltk.download('punkt_tab')\n from nltk.tokenize import word_tokenize\n from nltk.corpus import stopwords\n from nltk.stem import WordNetLemmatizer\n \n"
                },
                {
                    "date": 1742730393618,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n \n ## import libraries\n import pandas as pd\n import nltk\n-nltk.download('punkt_tab')\n+\n from nltk.tokenize import word_tokenize\n from nltk.corpus import stopwords\n from nltk.stem import WordNetLemmatizer\n \n"
                }
            ],
            "date": 1742726303582,
            "name": "Commit-0",
            "content": "## download packages\n#nltk.download('punkt_tab')\n#nltk.download('stopwords')\n#nltk.download('wordnet')\n\n## import libraries\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n\n## read the data\ndf_raw = pd.read_csv('Data/Raw/Job_Frauds.csv', encoding='latin-1')\n\n\n## split 'Job Location' into 'Country', 'State', 'City'\ndf = df_raw\nsplit_location = df['Job Location'].str.split(', ', expand=True)\ndf['Country'] = split_location[0]  \ndf['State'] = split_location[1] \ndf['City'] = split_location[2] \ndf = df.drop(columns=['Job Location'])\n\n## convert strings to lower case\ndf = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n\n## remove punctuations and symbols\ndf = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n\n## tokenize, remove stop words, and lemmatize for columns 'Profile', 'Job_Description', 'Requirements', 'Job_Benefits'\n# initialize stop words and lemmatizer\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()    \ncols = ['Profile', 'Job_Description', 'Requirements', 'Job_Benefits']\n\nfor col in cols:\n    # apply tokenization \n    df[col] = df[col].apply(lambda x: word_tokenize(x) if isinstance(x, str) and x.strip() != '' else [])\n    # remove stop words\n    df[col] = df[col].apply(lambda x: [word for word in x if word not in stop_words])\n    # apply lemmatization\n    df[col] = df[col].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n\nprint(df)\n\n## save as csv\n#df.to_csv('Data/Processed/processed_df.csv', index=False)"
        }
    ]
}